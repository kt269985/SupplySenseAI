{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üì¶ Import Packages and Connect Google Drive  \n",
        "\n",
        "Before we can work with data or build AI models, we need some **tools**.  \n",
        "In Python, these tools are called **packages (or libraries)**. Think of them like apps you download on your phone: each one has a special function.  \n",
        "\n",
        "- **pandas (pd):** Like Excel in Python ‚Äî helps us load and organize data tables.  \n",
        "- **numpy (np):** A calculator for fast math ‚Äî handles large sets of numbers.  \n",
        "- **matplotlib.pyplot (plt):** A drawing tool ‚Äî used to make graphs and plots.  \n",
        "- **scipy.stats (sp):** A toolbox for statistics ‚Äî helps with averages, probability, etc.  \n",
        "- **pywt:** A tool for signal analysis ‚Äî helps us break down data into patterns (wavelets).  \n",
        "\n",
        "We need to load these tools before we can use them in the notebook.  "
      ],
      "metadata": {
        "id": "xigBInMHvniD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MWwdLjHnopvm"
      },
      "outputs": [],
      "source": [
        "# Import basic packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as sp\n",
        "import pywt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîó Mount Google Drive  \n",
        "\n",
        "Google Colab runs in the cloud, but we often store our files (like data or reports) in Google Drive.  \n",
        "To make Colab read or save files directly to your Drive, we need to **‚Äúmount‚Äù** it.  \n",
        "You can think of this like plugging in a USB drive so your computer can access its files.  \n",
        "\n",
        "- When you run the code below, Colab will ask for your Google account permission.  \n",
        "- After granting access, your Drive will appear in the folder `/content/drive`.  \n",
        "- From now on, we can open files directly from your Drive or save results there.  "
      ],
      "metadata": {
        "id": "kyRctElvvzWf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSURm6XzqZNp"
      },
      "outputs": [],
      "source": [
        "# Mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ".\n",
        "\n",
        ".\n",
        "\n",
        "."
      ],
      "metadata": {
        "id": "JYikbjeXw_kK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÇ 1. Loading the Dataset from GitHub  \n",
        "\n",
        "Now we will load the dataset that we will use for our AI project.  \n",
        "This dataset was collected from a **Robotic Spot-Welding (RSW) process**.  \n",
        "\n",
        "- **Normal condition (180 samples):** Welding under proper conditions with a new tip.  \n",
        "- **Abnormal condition (180 samples):** Welding when the welding tip is worn out.  \n",
        "- Each sample contains signals from **three sensors**:  \n",
        "  - **Acceleration (vibration) signal**  \n",
        "  - **Voltage signal**  \n",
        "  - **Current signal**  \n",
        "\n",
        "So in total, we have **360 data samples** (180 Normal + 180 Abnormal)."
      ],
      "metadata": {
        "id": "r2lS2Fl5xDjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîé How the code works  \n",
        "\n",
        "- We use `pd.read_csv()` to read each dataset file from **GitHub** (https://github.com/ljwg3000/UNT_MEEN).  \n",
        "- The code below loads them as variables named **Normal_1 ~ Normal_180** and **Abnormal_1 ~ Abnormal_180**.  \n",
        "- Each variable contains the 3 sensor signals of one welding attempt.  "
      ],
      "metadata": {
        "id": "PEWoh6xYdHK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NoOfData = 180  # 180 Data for each robotic spot-welding condition (Normal, Abnormal)\n",
        "\n",
        "for i in range(NoOfData):\n",
        "\n",
        "    temp_path1 = f'https://github.com/ljwg3000/UNT_MEEN/blob/main/AI_tutorial/Dataset/Normal_{i+1}?raw=true'   # File path of normal dataset\n",
        "    temp_path2 = f'https://github.com/ljwg3000/UNT_MEEN/blob/main/AI_tutorial/Dataset/Abnormal_{i+1}?raw=true' # File path of abnormal dataset\n",
        "\n",
        "    exec(f\"Normal_{i+1}   = pd.read_csv(temp_path1 , sep=',' , header=None)\")\n",
        "    exec(f\"Abnormal_{i+1} = pd.read_csv(temp_path2 , sep=',' , header=None)\")"
      ],
      "metadata": {
        "id": "vu-iciOdxC-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä Example: Plotting Normal_1  \n",
        "\n",
        "As an example, the code below shows the signals from **Normal_1**:  \n",
        "\n",
        "- **Acceleration signal** (red)  \n",
        "- **Voltage signal** (green)  \n",
        "- **Current signal** (blue)  \n",
        "\n",
        "This helps us **visualize the raw sensor data** before doing any AI analysis.  "
      ],
      "metadata": {
        "id": "EiJn1xJFdL7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data = Normal_1\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "plt.subplot(3,1,1) # Acceleration signal\n",
        "plt.plot(Data.iloc[:,0] , Data.iloc[:,1], color='r')\n",
        "plt.ylabel('Acceleration (g)', fontsize=12, color='r')\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(3,1,2) # Voltage signal\n",
        "plt.plot(Data.iloc[:,0] , Data.iloc[:,2], color='g')\n",
        "plt.ylabel('Voltage (V)', fontsize=12, color='g')\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(3,1,3) # Current signal\n",
        "plt.plot(Data.iloc[:,0] , Data.iloc[:,3], color=[0,0,1])\n",
        "plt.ylabel('Current (kA)',fontsize=12, color='b')\n",
        "plt.xlabel('time (s)', fontsize=12)\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bYbIlUStb2qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".\n",
        "\n",
        ".\n",
        "\n",
        "."
      ],
      "metadata": {
        "id": "cNbW0rpZNOvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß© 2. Feature Extraction\n",
        "\n",
        "So far, we looked at **raw sensor signals** (acceleration, voltage, current).  \n",
        "But instead of using the raw signals directly, AI models usually perform better if we extract **features** ‚Äî  \n",
        "simple numbers that summarize each signal.  "
      ],
      "metadata": {
        "id": "bKJIjGwCNQnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå What are features?  \n",
        "A **feature** is like a \"summary number\" that represents an important characteristic of the signal.  \n",
        "For each sensor signal, we calculate **10 features**:  \n",
        "\n",
        "- **Max / Min values** (highest and lowest points)  \n",
        "- **Mean** (average)  \n",
        "- **RMS (Root Mean Square):** like a measure of the signal‚Äôs overall strength  \n",
        "- **Variance:** how spread out the values are  \n",
        "- **Skewness & Kurtosis:** describe the shape of the signal distribution  \n",
        "- **Crest factor, Shape factor, Impulse factor:** measures often used in vibration and fault detection  \n",
        "\n",
        "Since we have **3 sensors**, each data sample will have **30 features (3 √ó 10)**.  "
      ],
      "metadata": {
        "id": "A1wxlCrWfDeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Defining a function**\n",
        "\n",
        "One commonly used feature is the Root Mean Square (RMS) value.\n",
        "\n",
        "The **RMS** value of a signal \" $x = [x_1, x_2, \\dots, x_N]$ \" is defined as:\n",
        "\n",
        "\n",
        "\n",
        "$RMS(x) = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} x_i^2 }$\n",
        "\n",
        "\n",
        "where:  \n",
        "- $N$: number of samples  \n",
        "- $x_i$: the individual data points in the signal  \n",
        "\n",
        "üëâ In words: *square the values, take the mean, then take the square root.*\n",
        "\n",
        ".\n",
        "\n",
        "---\n",
        "\n",
        "We can define a simple function to calculate RMS in Python as following:"
      ],
      "metadata": {
        "id": "q3dyPu1SgA14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define RMS function\n",
        "def rms(x):\n",
        "    return np.sqrt(np.mean(x**2))"
      ],
      "metadata": {
        "id": "0xRcJO6RNIug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Step 2-1: Setting up empty feature arrays  \n",
        "- The code first creates empty arrays (`Feature_Normal`, `Feature_Abnormal`).  \n",
        "- These are like empty tables that will later be filled with the calculated features.  "
      ],
      "metadata": {
        "id": "vBIvkwuEfNoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NoOfSensor  = 3    # 3 Sensor signals: Acceleration, Voltage, Current\n",
        "NoOfFeature = 10   # 10 Feature types: Max, Min, Mean, RMS, Variance, Skewness, Kurtosis, Crest factor, Shape factor, Impulse factor\n",
        "\n",
        "# Create empty(0) arrays for normal/abnormal feature dataset (time domain)\n",
        "Feature_Normal   = np.zeros((NoOfSensor*NoOfFeature , NoOfData))\n",
        "Feature_Abnormal = np.zeros((NoOfSensor*NoOfFeature , NoOfData))\n",
        "\n",
        "print(Feature_Normal.shape)\n",
        "print(Feature_Abnormal.shape)\n",
        "\n",
        "Feature_Normal"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kiWEB9rcNhgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Step 2-2: Extracting features from each dataset  \n",
        "- The code loops through all **Normal** and **Abnormal** data samples.  \n",
        "- For each sample, and for each of the 3 sensor signals:  \n",
        "  - Calculate **10 features** (max, min, mean, RMS, etc.)  \n",
        "  - Store them in the corresponding array.  \n",
        "- This process transforms long raw signals into short, meaningful numbers.  "
      ],
      "metadata": {
        "id": "yQQgaCkUfQtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(NoOfData):\n",
        "\n",
        "    # Declare temporary data\n",
        "    exec(f\"temp_data1 = Normal_{i+1}\")\n",
        "    exec(f\"temp_data2 = Abnormal_{i+1}\")\n",
        "\n",
        "    # Time domain feature extraction\n",
        "    for j in range(NoOfSensor):\n",
        "\n",
        "        # Normal features\n",
        "        Feature_Normal[NoOfFeature*j+0, i] = np.max(temp_data1.iloc[:,j+1])\n",
        "        Feature_Normal[NoOfFeature*j+1, i] = np.min(temp_data1.iloc[:,j+1])\n",
        "        Feature_Normal[NoOfFeature*j+2, i] = np.mean(temp_data1.iloc[:,j+1])\n",
        "        Feature_Normal[NoOfFeature*j+3, i] = rms(temp_data1.iloc[:,j+1])\n",
        "        Feature_Normal[NoOfFeature*j+4, i] = np.var(temp_data1.iloc[:,j+1])\n",
        "        Feature_Normal[NoOfFeature*j+5, i] = sp.skew(temp_data1.iloc[:,j+1])\n",
        "        Feature_Normal[NoOfFeature*j+6, i] = sp.kurtosis(temp_data1.iloc[:,j+1])\n",
        "        Feature_Normal[NoOfFeature*j+7, i] = np.max(temp_data1.iloc[:,j+1])/rms(temp_data1.iloc[:,j+1])\n",
        "        Feature_Normal[NoOfFeature*j+8, i] = rms(temp_data1.iloc[:,j+1])/np.mean(np.abs(temp_data1.iloc[:,j+1]))\n",
        "        Feature_Normal[NoOfFeature*j+9, i] = np.max(temp_data1.iloc[:,j+1])/np.mean(np.abs(temp_data1.iloc[:,j+1]))\n",
        "\n",
        "        # Abnormal features\n",
        "        Feature_Abnormal[NoOfFeature*j+0, i] = np.max(temp_data2.iloc[:,j+1])\n",
        "        Feature_Abnormal[NoOfFeature*j+1, i] = np.min(temp_data2.iloc[:,j+1])\n",
        "        Feature_Abnormal[NoOfFeature*j+2, i] = np.mean(temp_data2.iloc[:,j+1])\n",
        "        Feature_Abnormal[NoOfFeature*j+3, i] = rms(temp_data2.iloc[:,j+1])\n",
        "        Feature_Abnormal[NoOfFeature*j+4, i] = np.var(temp_data2.iloc[:,j+1])\n",
        "        Feature_Abnormal[NoOfFeature*j+5, i] = sp.skew(temp_data2.iloc[:,j+1])\n",
        "        Feature_Abnormal[NoOfFeature*j+6, i] = sp.kurtosis(temp_data2.iloc[:,j+1])\n",
        "        Feature_Abnormal[NoOfFeature*j+7, i] = np.max(temp_data2.iloc[:,j+1])/rms(temp_data2.iloc[:,j+1])\n",
        "        Feature_Abnormal[NoOfFeature*j+8, i] = rms(temp_data2.iloc[:,j+1])/np.mean(np.abs(temp_data2.iloc[:,j+1]))\n",
        "        Feature_Abnormal[NoOfFeature*j+9, i] = np.max(temp_data2.iloc[:,j+1])/np.mean(np.abs(temp_data2.iloc[:,j+1]))\n",
        "\n",
        "print(Feature_Normal.shape)\n",
        "print(Feature_Abnormal.shape)\n",
        "\n",
        "Feature_Normal"
      ],
      "metadata": {
        "id": "MWqSyXC0NriM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Step 2-3: Combining Normal and Abnormal features  \n",
        "- After extracting, we combine the two arrays into one dataset:  \n",
        "  - **Normal features**  \n",
        "  - **Abnormal features**  \n",
        "- Now we have a single dataset (`FeatureData`) that can be used for AI training.  \n"
      ],
      "metadata": {
        "id": "HHIQmYDbO7Zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FeatureData = pd.DataFrame(np.concatenate([Feature_Normal, Feature_Abnormal] , axis=1))\n",
        "FeatureData.shape"
      ],
      "metadata": {
        "id": "95rkNhIIO7mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üíæ Step 2-4: Saving the feature dataset  \n",
        "- Finally, the dataset is saved as a `.csv` file in Google Drive.  \n",
        "- This file contains all the features we will use to train and test our AI model.  "
      ],
      "metadata": {
        "id": "uQNlK4QaPWFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/FeatureData.csv'\n",
        "FeatureData.to_csv(path, sep=',', header=None , index=None)"
      ],
      "metadata": {
        "id": "nfKTadhsPWON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".\n",
        "\n",
        ".\n",
        "\n",
        "."
      ],
      "metadata": {
        "id": "stv7Lr6qxBSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä 3. Preparing Data and Labels for Machine Learning  \n",
        "\n",
        "Once we have extracted features, the next step is to prepare them for training a machine learning model.  \n",
        "This process has several important steps:"
      ],
      "metadata": {
        "id": "dYt0fNYkhvww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Step 3-1. Standardizing Features  \n",
        "Different features may have very different scales (for example, current in thousands vs. voltage in tens).  \n",
        "To make sure all features contribute equally to the learning process, we **standardize** them using `StandardScaler`.  \n",
        "This rescales all features so that they have similar ranges, improving both the speed and accuracy of training.\n"
      ],
      "metadata": {
        "id": "hPYwnvNnVbe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize feature values\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "FeatureData_std = StandardScaler().fit_transform(FeatureData.T)\n",
        "FeatureData_std.shape"
      ],
      "metadata": {
        "id": "1zJYZ3hFZz7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Step 3-2. Splitting Training and Test Sets  \n",
        "To evaluate the performance of our model, we divide the dataset into two parts:  \n",
        "\n",
        "- **Training set (80%)**: Used to train the model.  \n",
        "- **Test set (20%)**: Used to evaluate how well the model performs on unseen data.  \n",
        "\n",
        "We use the `train_test_split` function from **scikit-learn** to randomly split the data.  \n",
        "A fixed `random_state` ensures that the split is reproducible."
      ],
      "metadata": {
        "id": "9YTkdG9Yibeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of data for each condition: 180\n",
        "NormalSet   = FeatureData_std[:NoOfData , :]\n",
        "AbnormalSet = FeatureData_std[NoOfData: , :]\n",
        "\n",
        "NormalSet.shape, AbnormalSet.shape"
      ],
      "metadata": {
        "id": "sSHeYisJX8SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Designate test data ratio\n",
        "TestData_Ratio = 0.2\n",
        "\n",
        "TrainData_Nor, TestData_Nor = train_test_split(NormalSet  , test_size=TestData_Ratio, random_state=777)\n",
        "TrainData_Abn, TestData_Abn = train_test_split(AbnormalSet, test_size=TestData_Ratio, random_state=777)\n",
        "\n",
        "print(TrainData_Nor.shape, TestData_Nor.shape)\n",
        "print(TrainData_Abn.shape, TestData_Abn.shape)"
      ],
      "metadata": {
        "id": "T_-DpjKpX7-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Step 3-3. Creating Labels with One-Hot Encoding  \n",
        "Machine learning models need labels that tell whether each sample is **Normal** or **Abnormal**.  \n",
        "We use **one-hot encoding**, which represents categories as binary vectors:  \n",
        "\n",
        "- `[1, 0]` ‚Üí Normal  \n",
        "- `[0, 1]` ‚Üí Abnormal  \n",
        "\n",
        "This format is easier for neural networks to process.  \n",
        "Labels are created separately for training and test datasets using `np.zeros` and `np.ones`."
      ],
      "metadata": {
        "id": "13A3jjjFimuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TrainLabel_Nor = np.zeros((TrainData_Nor.shape[0],2))\n",
        "TrainLabel_Abn = np.ones( (TrainData_Abn.shape[0],2))\n",
        "TestLabel_Nor  = np.zeros((TestData_Nor.shape[0],2))\n",
        "TestLabel_Abn  = np.ones( (TestData_Abn.shape[0],2))\n",
        "\n",
        "TrainLabel_Nor[:,0] = 1  # [1,0]: Normal\n",
        "TrainLabel_Abn[:,0] = 0  # [0,1]: Abnormal\n",
        "TestLabel_Nor[:,0]  = 1  # [1,0]: Normal\n",
        "TestLabel_Abn[:,0]  = 0  # [0,1]: Abnormal\n",
        "\n",
        "print(TrainLabel_Nor.shape, TestLabel_Nor.shape)\n",
        "print(TrainLabel_Abn.shape, TestLabel_Abn.shape)"
      ],
      "metadata": {
        "id": "j4-2rzxQboWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a label set\n",
        "TestLabel_Nor"
      ],
      "metadata": {
        "id": "nBBQfx_uxQO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Step 3-4. Combining Data and Labels  \n",
        "Finally, we merge the Normal and Abnormal sets together:  \n",
        "\n",
        "- `TrainData` and `TestData` hold all feature values.  \n",
        "- `TrainLabel` and `TestLabel` hold the corresponding one-hot encoded labels.  \n",
        "\n",
        "Now, both the input features and target labels are ready for training and evaluating the machine learning model.  \n"
      ],
      "metadata": {
        "id": "zNHeUOUGi63c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TrainData  = np.concatenate([TrainData_Nor , TrainData_Abn ], axis=0)\n",
        "TestData   = np.concatenate([TestData_Nor  , TestData_Abn  ], axis=0)\n",
        "TrainLabel = np.concatenate([TrainLabel_Nor, TrainLabel_Abn], axis=0)\n",
        "TestLabel  = np.concatenate([TestLabel_Nor , TestLabel_Abn ], axis=0)\n",
        "\n",
        "print(TrainData.shape,  TestData.shape)\n",
        "print(TrainLabel.shape, TestLabel.shape)"
      ],
      "metadata": {
        "id": "HnZh6FJKX7Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "."
      ],
      "metadata": {
        "id": "Ea-DpZP9TcaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ 4. AI Modeling with MLP (Multi-Layer Perceptron)\n",
        "\n",
        "Now that we have prepared our dataset, we move to the **AI modeling stage**.  \n",
        "Here we build, train, and evaluate a simple **neural network model** using TensorFlow and Keras.\n",
        "\n"
      ],
      "metadata": {
        "id": "o4rTcskJju7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Step 4-1. Importing TensorFlow\n",
        "- We first import **TensorFlow**, one of the most widely used deep learning frameworks.  \n",
        "- TensorFlow (with Keras) provides tools to easily build and train neural networks.  \n",
        "- We also check the TensorFlow version to ensure compatibility."
      ],
      "metadata": {
        "id": "I0NeOyp4g7oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'Tensorflow' pakage\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Check the version of tensorflow\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "PQipZlHLg8Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwgRfDG6opvn"
      },
      "source": [
        "### ‚öôÔ∏è Step 4-2. Setting Hyperparameters\n",
        "Before training, we define **hyperparameters**, which control how the model learns:\n",
        "- **learningRate**: how fast the model updates during training.  \n",
        "- **noOfNeuron**: number of neurons in each hidden layer.  \n",
        "- **Epoch**: how many times the model sees the entire dataset during training.  \n",
        "\n",
        "These values affect both the **speed** and **accuracy** of learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBYZBbydopvn"
      },
      "outputs": [],
      "source": [
        "learningRate  = 0.0001\n",
        "noOfNeuron    = 16\n",
        "Epoch         = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Step 4-3. Designing the MLP Model\n",
        "We design a neural network using **Keras Sequential API**:\n",
        "- **Input Layer**: matches the number of features in the dataset.  \n",
        "- **Hidden Layers**: use ReLU activation to capture non-linear patterns.  \n",
        "- **Output Layer**: uses Softmax activation with 2 neurons (Normal / Abnormal classification).  \n",
        "\n",
        "The model is compiled with:\n",
        "- **Optimizer**: Adam (adaptive learning optimizer).  \n",
        "- **Loss Function**: Categorical Crossentropy (for classification).  \n",
        "- **Metric**: Accuracy (to measure performance).\n"
      ],
      "metadata": {
        "id": "-qwBTEDzkuwU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yCi-yvzopvn"
      },
      "source": [
        "- Types of Activation Functions: https://keras.io/api/layers/activations/\n",
        "\n",
        "- Types of Optimization Algorithms: https://keras.io/api/optimizers/\n",
        "\n",
        "- Types of Loss Functions (for Classification)  https://keras.io/api/losses/probabilistic_losses/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZvrun9Oopvo"
      },
      "outputs": [],
      "source": [
        "def MLP_model(input_data):\n",
        "    keras.backend.clear_session() # clearing the Keras backend session (initiating variables)\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.InputLayer(input_shape = (input_data.shape[1],) ))                                      # Input  Layer\n",
        "    model.add(keras.layers.Dense(units = noOfNeuron, activation = keras.activations.relu,    name = 'Hidden1'))    # Hidden Layer 1\n",
        "    model.add(keras.layers.Dense(units = noOfNeuron, activation = keras.activations.relu,    name = 'Hidden2'))    # Hidden Layer 2\n",
        "    model.add(keras.layers.Dense(units = 2,          activation = keras.activations.softmax, name = 'Output'))     # Output Layer\n",
        "\n",
        "    model.compile(optimizer = keras.optimizers.Adam(learning_rate = learningRate), # Optimization algorithm\n",
        "                  loss = keras.losses.CategoricalCrossentropy(),                   # Loss function (objective function of Optimization)\n",
        "                  metrics = ['accuracy'])                                          # Metrics to measure during the training process\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWxSRDntopvo"
      },
      "outputs": [],
      "source": [
        "# Check the model architecture and the number of parameters\n",
        "MLP = MLP_model(TrainData)\n",
        "MLP.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lGSQOni2k--"
      },
      "outputs": [],
      "source": [
        "# Check the parameter shape for each layer\n",
        "for i in range(len(MLP.get_weights())):\n",
        "    print(MLP.get_weights()[i].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gISsVxFAopvo"
      },
      "source": [
        "### ‚öôÔ∏è Step 4-4. Training the Model\n",
        "- The model is trained using the training dataset and labels.  \n",
        "- The network updates its weights over multiple epochs to minimize the **loss function**.  \n",
        "- Training history (loss & accuracy) is recorded for later visualization.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SNLBAx1opvo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(777) # Not necessarily required\n",
        "\n",
        "# Model traning and validation\n",
        "TraingHistory  = MLP.fit(TrainData, TrainLabel, epochs=Epoch, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Step 4-5. Evaluating the Model\n",
        "- After training, we test the model on the **test dataset** (data the model has never seen).  \n",
        "- **Loss** shows how far predictions are from the true labels (lower is better).  \n",
        "- **Accuracy** shows how many samples are correctly classified (closer to 100% is better)."
      ],
      "metadata": {
        "id": "m5_pmJUIZ1xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation result for test data (not trained)\n",
        "Loss, Accuracy = MLP.evaluate(TestData,  TestLabel, verbose=0)\n",
        "Loss, Accuracy # The closer the Loss is to 0 and the closer the accuracy is to 1 (100%), the better."
      ],
      "metadata": {
        "id": "0B1trRtcZ7js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Step 4-6. Visualizing Training Progress\n",
        "- We plot **loss and accuracy vs. epochs** to check if the model is learning properly.  \n",
        "- Smooth decrease in loss and increase in accuracy ‚Üí good learning.  \n",
        "- If accuracy improves only on training data but not on test data ‚Üí possible **overfitting**."
      ],
      "metadata": {
        "id": "AYB8xM-3Z35F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AmVqsTnkzlw"
      },
      "outputs": [],
      "source": [
        "# Check the training process (Loss, Accuracy)\n",
        "\n",
        "fig, loss_ax = plt.subplots(figsize=(8,6))\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(TraingHistory.history['loss'], label='train loss', c = 'tab:red')\n",
        "loss_ax.set_xlabel('epoch', fontsize=15)\n",
        "loss_ax.set_ylabel('loss', fontsize=15)\n",
        "loss_ax.legend(loc='upper left', fontsize=15)\n",
        "\n",
        "acc_ax.plot(TraingHistory.history['accuracy'], label='train acc', c = 'tab:blue')\n",
        "acc_ax.set_ylabel('accuracy', fontsize=15)\n",
        "acc_ax.legend(loc='lower left', fontsize=15)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Step 4-7. Saving and Loading the Model\n",
        "- Once trained, the model is saved as a `.keras` file for later use.  \n",
        "- We can reload the saved model without retraining, and use it to make predictions."
      ],
      "metadata": {
        "id": "8WbOlv7YZmFd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgflb5V4opvq"
      },
      "outputs": [],
      "source": [
        "MLP.save('/content/drive/MyDrive/Colab Notebooks/MLP_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-3monSeopvq"
      },
      "outputs": [],
      "source": [
        "LoadedModel = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/MLP_model.keras')\n",
        "\n",
        "Loss, Accuracy = LoadedModel.evaluate(TestData, TestLabel, verbose=0)\n",
        "print('[Performance of ANN model] \\n')\n",
        "print('Accuracy : {:.2f}%'.format(Accuracy*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1pcU18Gopvq",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Predicted result\n",
        "Predicted = LoadedModel.predict(TestData)\n",
        "pd.DataFrame(Predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Step 4-8. Evaluating with Confusion Matrix\n",
        "- Accuracy alone may not fully describe performance.  \n",
        "- A **confusion matrix** shows detailed results:\n",
        "  - **True Positive (TP):** Abnormal correctly predicted as Abnormal.  \n",
        "  - **True Negative (TN):** Normal correctly predicted as Normal.  \n",
        "  - **False Positive (FP):** Normal incorrectly predicted as Abnormal.  \n",
        "  - **False Negative (FN):** Abnormal incorrectly predicted as Normal.  \n",
        "\n",
        "This helps us understand the model‚Äôs strengths and weaknesses in more detail."
      ],
      "metadata": {
        "id": "Iv-C1uKPcUWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Convert TestLabel and Predicted into single-column vectors for evaluations\n",
        "TestLabel_rev = np.argmax(TestLabel, axis=1)\n",
        "Predicted_rev = np.argmax(Predicted, axis=1)\n",
        "\n",
        "cm = confusion_matrix(TestLabel_rev, Predicted_rev)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False, square=True)\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.ylabel(\"True label\")\n",
        "plt.title(\"Confusion Matrix of the MLP Model\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A4d8ISYxcYzV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}